"""LinkedIn sourcer using web scraping (no API required)."""

from typing import List, Optional, Dict, Any
from datetime import datetime
import re

try:
    from bs4 import BeautifulSoup
    import aiohttp
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

from .base import BaseSourcer, SourcedContent


class LinkedInSourcer(BaseSourcer):
    """
    LinkedIn public post sourcer using web scraping.
    
    NOTE: LinkedIn actively blocks scrapers. This implementation works for
    PUBLIC posts only and may require:
    - Rotating user agents
    - Proxy rotation
    - Rate limiting
    - Cookie handling
    
    For production, consider:
    1. LinkedIn Official API (requires approval + partnership)
    2. Third-party APIs like Proxycurl, RapidAPI
    3. Browser automation with Playwright/Selenium
    
    This basic implementation is for educational purposes and light usage.
    """

    def __init__(
        self,
        search_query: Optional[str] = None,
        company_id: Optional[str] = None,
        name: Optional[str] = None,
        max_posts: int = 20,
    ):
        """
        Initialize LinkedIn sourcer.

        Args:
            search_query: Search keywords
            company_id: LinkedIn company ID for company posts
            name: Optional name for this sourcer
            max_posts: Maximum number of posts to fetch (default: 20)
        """
        super().__init__(name)
        
        if not BS4_AVAILABLE:
            raise ImportError(
                "beautifulsoup4 and aiohttp are required. "
                "Install with: pip install beautifulsoup4 aiohttp"
            )
        
        self.search_query = search_query
        self.company_id = company_id
        self.max_posts = max_posts
        
        self.validate_config()

    def validate_config(self, **kwargs) -> bool:
        """Validate LinkedIn sourcer configuration."""
        if not self.search_query and not self.company_id:
            raise ValueError("Either search_query or company_id is required")
        
        return True

    async def fetch(self, **kwargs) -> List[SourcedContent]:
        """
        Fetch public posts from LinkedIn.

        Args:
            **kwargs: Optional override parameters

        Returns:
            List of SourcedContent objects
            
        Note:
            This is a basic implementation. LinkedIn heavily rate-limits and blocks
            scrapers. For production, use official API or third-party services.
        """
        max_posts = kwargs.get("max_posts", self.max_posts)
        
        contents = []
        
        # Warning message
        print(
            "âš ï¸  LinkedIn scraping is limited and may not work reliably. "
            "Consider using LinkedIn Official API or services like Proxycurl."
        )
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                         "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        }
        
        try:
            async with aiohttp.ClientSession(headers=headers) as session:
                # For now, return empty list with informational message
                # Real implementation would need sophisticated anti-detection
                
                # Example structure for when implementing:
                # if self.search_query:
                #     url = f"https://www.linkedin.com/search/results/content/?keywords={self.search_query}"
                # elif self.company_id:
                #     url = f"https://www.linkedin.com/company/{self.company_id}/posts/"
                
                # Placeholder content to show structure
                sourced_content = SourcedContent(
                    title="LinkedIn API Notice",
                    content=(
                        "LinkedIn scraping requires sophisticated anti-detection measures. "
                        "For production use, please implement one of these solutions:\n\n"
                        "1. LinkedIn Official API (https://developer.linkedin.com/)\n"
                        "2. Proxycurl API (https://nubela.co/proxycurl/)\n"
                        "3. RapidAPI LinkedIn endpoints\n"
                        "4. Browser automation with Playwright/Selenium\n\n"
                        "This sourcer is a placeholder for your implementation."
                    ),
                    url="https://developer.linkedin.com/",
                    published_date=datetime.now(),
                    author="System",
                    metadata={
                        "platform": "linkedin",
                        "status": "placeholder",
                        "note": "Requires API implementation",
                    },
                )
                
                contents.append(sourced_content)
        
        except Exception as e:
            raise Exception(f"Failed to fetch from LinkedIn: {str(e)}")
        
        return contents

    def __repr__(self) -> str:
        if self.company_id:
            return f"<LinkedInSourcer: {self.name} (Company ID: {self.company_id})>"
        return f"<LinkedInSourcer: {self.name} ({self.search_query})>"


class ProxycurlLinkedInSourcer(BaseSourcer):
    """
    LinkedIn sourcer using Proxycurl API (commercial service).
    
    Proxycurl provides a robust LinkedIn API with:
    - Company posts and updates
    - Profile data
    - Job postings
    - Company information
    
    Pricing: Pay-as-you-go, starts at $0.01 per request
    Sign up: https://nubela.co/proxycurl/
    
    Set PROXYCURL_API_KEY environment variable or pass as parameter.
    """

    def __init__(
        self,
        company_url: str,
        api_key: Optional[str] = None,
        name: Optional[str] = None,
        max_posts: int = 20,
    ):
        """
        Initialize Proxycurl LinkedIn sourcer.

        Args:
            company_url: LinkedIn company URL
            api_key: Proxycurl API key (or set PROXYCURL_API_KEY env var)
            name: Optional name for this sourcer
            max_posts: Maximum posts to fetch
        """
        super().__init__(name)
        
        import os
        self.company_url = company_url
        self.api_key = api_key or os.getenv("PROXYCURL_API_KEY")
        self.max_posts = max_posts
        
        self.validate_config()

    def validate_config(self, **kwargs) -> bool:
        """Validate configuration."""
        if not self.api_key:
            raise ValueError(
                "Proxycurl API key required. Set PROXYCURL_API_KEY environment variable "
                "or pass as parameter. Sign up at: https://nubela.co/proxycurl/"
            )
        
        if not self.company_url:
            raise ValueError("company_url is required")
        
        return True

    async def fetch(self, **kwargs) -> List[SourcedContent]:
        """
        Fetch company posts using Proxycurl API.

        Returns:
            List of SourcedContent objects
        """
        contents = []
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
        }
        
        try:
            async with aiohttp.ClientSession(headers=headers) as session:
                # Get company updates
                url = "https://nubela.co/proxycurl/api/linkedin/company/updates"
                params = {
                    "url": self.company_url,
                }
                
                async with session.get(url, params=params) as response:
                    if response.status != 200:
                        raise Exception(f"Proxycurl API error: {response.status}")
                    
                    data = await response.json()
                    
                    updates = data.get("updates", [])[:self.max_posts]
                    
                    for update in updates:
                        # Parse update data
                        text = update.get("text", "")
                        link = update.get("url", "")
                        timestamp = update.get("posted_on", {})
                        
                        # Parse date
                        published_date = None
                        if timestamp:
                            published_date = datetime.fromtimestamp(timestamp.get("timestamp", 0))
                        
                        metadata = {
                            "platform": "linkedin",
                            "post_url": link,
                            "company_url": self.company_url,
                            "num_likes": update.get("num_likes", 0),
                            "num_comments": update.get("num_comments", 0),
                            "num_shares": update.get("num_shares", 0),
                            "total_reactions": update.get("total_reactions", 0),
                        }
                        
                        sourced_content = SourcedContent(
                            title=f"LinkedIn Post - {update.get('author', {}).get('name', 'Unknown')}",
                            content=text,
                            url=link,
                            published_date=published_date,
                            author=update.get("author", {}).get("name", "Unknown"),
                            metadata=metadata,
                        )
                        
                        contents.append(sourced_content)
        
        except Exception as e:
            raise Exception(f"Failed to fetch from Proxycurl: {str(e)}")
        
        return contents

    def __repr__(self) -> str:
        return f"<ProxycurlLinkedInSourcer: {self.name} ({self.company_url})>"
