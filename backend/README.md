# Perceptron Backend# Signal Radar Backend



Modular NLP pipeline for monitoring RSS feeds, extracting keywords, and tracking trends across different team perspectives.FastAPI-based REST API for the Deutsche Bank Signal Radar intelligence dashboard with a complete **data ingestion and storage pipeline** for NLP/ML processing.



## Quick Start## ğŸš€ Features



```bash- **Modular Sourcer Pipeline** - Fetch content from RSS feeds, web pages, APIs

# Install dependencies- **Data Lake** - Persistent storage with automatic deduplication

pip install -r requirements.txt- **Perpetual Monitoring** - Scheduler for automatic content fetching

- **NLP-Ready** - Optimized for TF-IDF, sentiment analysis, entity detection

# Download spaCy model- **REST API** - Full API for data lake management

python -m spacy download en_core_web_md

## Quick Start (Hackathon Mode ğŸƒâ€â™‚ï¸)

# Initialize databases from config.json

python scripts/setup_teams.py init```bash

python scripts/setup_data_lake.py init# Make scripts executable (first time only)

python scripts/setup_keywords.py initchmod +x run.sh format.sh



# Fetch initial data# Start the backend (handles venv, dependencies, and server startup)

python scripts/setup_data_lake.py fetch./run.sh

```

# Run the application

./run.shThe API will be available at:

```

- **API:** `http://localhost:8000`

## Project Structure- **Interactive Docs:** `http://localhost:8000/docs`

- **Alternative Docs:** `http://localhost:8000/redoc`

```

backend/## Manual Setup

â”œâ”€â”€ app.py                      # FastAPI application (main entry point)

â”œâ”€â”€ config.json                 # Single source of truth for teams & sourcesIf you prefer to set up manually:

â”œâ”€â”€ api_models.py               # Pydantic models for API responses

â”œâ”€â”€ scheduler.py                # Perpetual monitoring scheduler```bash

â”‚# Create virtual environment

â”œâ”€â”€ sourcers/                   # Content sourcing modulespython3 -m venv venv

â”‚   â”œâ”€â”€ base.py                 # Base sourcer interface

â”‚   â””â”€â”€ rss_sourcer.py          # RSS feed implementation# Activate it

â”‚source venv/bin/activate  # On macOS/Linux

â”œâ”€â”€ storage/                    # Data lake storage# or

â”‚   â”œâ”€â”€ models.py               # Content & source config modelsvenv\Scripts\activate     # On Windows

â”‚   â””â”€â”€ repository.py           # Database operations

â”‚# Install dependencies (including dev dependencies)

â”œâ”€â”€ keywords/                   # Keyword extraction & analysispip install -e ".[dev]"

â”‚   â”œâ”€â”€ models.py               # Keyword database models

â”‚   â”œâ”€â”€ extractor.py            # Multi-method extraction (TF-IDF, spaCy, YAKE)# Run the server

â”‚   â”œâ”€â”€ processor.py            # Real-time processing pipelineuvicorn app:app --reload --port 8000

â”‚   â”œâ”€â”€ repository.py           # Keyword database operations```

â”‚   â””â”€â”€ importance_models.py    # Importance & sentiment tracking

â”‚## API Endpoints

â”œâ”€â”€ teams/                      # Team-based configuration

â”‚   â”œâ”€â”€ models.py               # Team & source models### Core Endpoints

â”‚   â””â”€â”€ repository.py           # Team data access

â”‚- `GET /` - Root endpoint

â”œâ”€â”€ data/                       # SQLite databases (generated)- `GET /api/health` - Health check

â”‚   â”œâ”€â”€ sourcer_pipeline.db     # Content & sources- `GET /api/hello` - Hello world example

â”‚   â”œâ”€â”€ keywords.db             # Extracted keywords

â”‚   â””â”€â”€ teams.db                # Team configurations### Sourcer Pipeline Endpoints

â”‚

â”œâ”€â”€ mock_data/                  # Mock JSON for frontend development- `POST /api/sources/rss/fetch` - Fetch entries from any RSS feed

â”‚- `GET /api/sources/rss/example` - Fetch from example RSS feed (TechCrunch)

â”œâ”€â”€ scripts/                    # Setup, test, and demo scripts

â”‚   â”œâ”€â”€ setup_teams.py          # Initialize teams from config.json### Data Lake Endpoints

â”‚   â”œâ”€â”€ setup_data_lake.py      # Initialize content database

â”‚   â”œâ”€â”€ setup_keywords.py       # Initialize keywords database- `POST /api/datalake/sources/add` - Add source to monitor

â”‚   â”œâ”€â”€ test_*.py               # Test scripts- `GET /api/datalake/sources/list` - List all configured sources

â”‚   â””â”€â”€ demo_*.py               # Demo scripts- `POST /api/datalake/fetch-and-store` - Fetch and store with deduplication

â”‚- `GET /api/datalake/stats` - Get database statistics

â””â”€â”€ docs/                       # Documentation- `GET /api/datalake/content/unprocessed` - Get content ready for NLP processing

    â”œâ”€â”€ SINGLE_SOURCE_OF_TRUTH.md  # Config.json guide- `POST /api/datalake/content/{id}/mark-processed` - Mark content as processed

    â”œâ”€â”€ CONFIG_GUIDE.md            # Complete configuration reference

    â”œâ”€â”€ FRONTEND_GUIDE.md          # API documentation for frontendğŸ“– **See [DATA_LAKE.md](DATA_LAKE.md) for comprehensive data lake documentation.**  

    â”œâ”€â”€ FRONTEND_QUICKSTART.md     # Quick frontend integrationğŸ“– **See [SOURCERS.md](SOURCERS.md) for detailed sourcer pipeline documentation.**  

    â””â”€â”€ TEAM_SYSTEM_SUMMARY.md     # Architecture overviewğŸ“– **See [SUMMARY.md](SUMMARY.md) for implementation summary.**

```

## Data Lake & NLP Pipeline

## Key Features

### Initialize Data Lake

### 1. Single Source of Truth (`config.json`)

All teams, RSS sources, and keyword extraction settings are defined in one JSON file:```bash

- 4 teams: Regulatory, Investment, Competitive Intelligence, Research# Create database and add example sources

- 13 RSS feed sourcespython3 setup_data_lake.py init

- Team-specific keyword extraction thresholds

- Team-specific sentiment analysis settings# Fetch from all configured sources

python3 setup_data_lake.py fetch

See: `docs/SINGLE_SOURCE_OF_TRUTH.md`

# View statistics

### 2. Modular Sourcing Pipelinepython3 setup_data_lake.py stats

- RSS feed scraping with `feedparser````

- Automatic deduplication (SHA-256 content hashing)

- Perpetual monitoring with configurable intervals### Perpetual Monitoring

- Metadata tracking (published date, author, tags)

```bash

See: `docs/SOURCERS.md`# Run scheduler for automatic periodic fetching

python3 scheduler.py

### 3. Sophisticated Keyword Extraction```

- Multi-method approach: TF-IDF, spaCy NLP, YAKE

- Multi-word phrase detectionThe scheduler will:

- Stopword filtering

- Team-specific relevance thresholds- Automatically fetch from configured sources on schedule

- Real-time processing as content arrives- Deduplicate content using SHA-256 hashing

- Track statistics and errors

See: `docs/KEYWORD_EXTRACTION.md`- Update next fetch times



### 4. Importance & Sentiment Tracking### Process Content for NLP

- Multi-signal importance scoring:

  - Frequency (30%)```bash

  - Velocity (25%)# Run example NLP processing

  - Source diversity (20%)python3 example_nlp_processing.py process

  - Recency (15%)

  - Sentiment magnitude (10%)# Analyze trends

- Sentiment analysis (VADER/TextBlob)python3 example_nlp_processing.py trends

- Time-series data for trending```



See: `docs/TEAM_SYSTEM_SUMMARY.md`**Ready for:**



### 5. Team-Based Filtering- TF-IDF word frequency analysis

- Each team sees only relevant sources- Sentiment analysis

- Different keyword extraction settings per team- Entity detection

- Proactive threat/trend detection- Topic modeling

- Word cloud visualization support- Any NLP/ML pipeline



See: `docs/CONFIG_GUIDE.md`## Sourcer Pipeline



## ConfigurationThe backend includes a modular data sourcing pipeline that can fetch content from various sources. Currently supports:



### Edit Teams & Sources- **RSS Feeds** - Fetch articles from any RSS/Atom feed



```bash### Quick Example

# 1. Edit the single source of truth

vim config.json```python

from sourcers import RSSSourcer

# 2. Validate changes

python scripts/setup_teams.py validatesourcer = RSSSourcer(

    feed_url="https://techcrunch.com/feed/",

# 3. Apply changes    max_entries=10

python scripts/setup_teams.py init)

```

contents = await sourcer.fetch()

### Add a New Teamfor content in contents:

    print(f"{content.title} - {content.url}")

Add to `config.json`:```

```json

{### Test the Pipeline

  "team_key": "legal",

  "team_name": "Legal Team",```bash

  "description": "Monitors legal developments",# Run the example script

  "color": "#DC2626",python3 example_usage.py

  "icon": "gavel",

  "is_active": true,# Run tests

  "keyword_config": {python3 test_sourcers.py

    "relevance_threshold": 0.55,```

    "min_frequency": 2,

    "max_keywords_per_day": 60,## Code Formatting

    "enable_multi_word_phrases": true,

    "max_phrase_length": 4,This project uses **black** and **isort** for Python code formatting.

    "filter_stopwords": true,

    "methods": ["tfidf", "spacy"]### Format Code

  },

  "sentiment_config": {```bash

    "enable_sentiment": true,./format.sh

    "sentiment_method": "vader",```

    "importance_weight": 0.1

  },Or manually:

  "sources": [...]

}```bash

```isort .

black .

Then run: `python scripts/setup_teams.py init````



## API Endpoints (Planned)### Pre-commit Hooks (Optional)



```To automatically format code before commits:

GET  /api/teams                              # List all teams

GET  /api/keywords/wordcloud?team={key}      # Get word cloud data```bash

GET  /api/keywords/timeseries?keyword={word} # Get time-series datapre-commit install

GET  /api/sources                            # List all sources```

POST /api/content/fetch                      # Trigger manual fetch

```## Development



See: `docs/FRONTEND_GUIDE.md`The server runs with hot-reload enabled by default. Just save your files and the server will restart automatically.



## Development### Key Features



### Run Tests- âš¡ **FastAPI** - Modern, fast, async Python web framework

```bash- ğŸ“š **Auto-generated docs** - Interactive API documentation at `/docs`

python scripts/test_sourcers.py- ğŸ”„ **Hot reload** - Changes reflected immediately

python scripts/test_keyword_extraction.py- ğŸ¨ **Formatted code** - Black & isort configured via pyproject.toml

python scripts/test_team_config.py

```## Project Structure



### View Statistics```

```bashbackend/

python scripts/setup_data_lake.py statsâ”œâ”€â”€ app.py                          # Main FastAPI application

python scripts/setup_keywords.py statsâ”œâ”€â”€ sourcers/                       # Data sourcing pipeline

python scripts/setup_teams.py statsâ”‚   â”œâ”€â”€ __init__.py                # Package exports

```â”‚   â”œâ”€â”€ base.py                    # BaseSourcer and SourcedContent

â”‚   â”œâ”€â”€ rss_sourcer.py             # RSS feed implementation

### Reset Everythingâ”‚   â””â”€â”€ template_sourcer.py        # Template for new sourcers

```bashâ”œâ”€â”€ storage/                        # Data lake storage layer

python scripts/reset_databases.pyâ”‚   â”œâ”€â”€ __init__.py                # Package exports

```â”‚   â”œâ”€â”€ models.py                  # Database models

â”‚   â””â”€â”€ repository.py              # Data access layer

## Tech Stackâ”œâ”€â”€ data/                           # SQLite database

â”‚   â””â”€â”€ sourcer_pipeline.db

- **FastAPI** 0.109.0 - REST API frameworkâ”œâ”€â”€ scheduler.py                    # Perpetual fetching service

- **SQLAlchemy** 2.0.44 - ORM for SQLite databasesâ”œâ”€â”€ setup_data_lake.py              # Data lake setup & management

- **feedparser** 6.0.11 - RSS/Atom feed parsingâ”œâ”€â”€ example_usage.py                # Sourcer usage examples

- **scikit-learn** - TF-IDF keyword extractionâ”œâ”€â”€ example_nlp_processing.py       # NLP processing examples

- **spaCy** - NLP (named entities, noun phrases)â”œâ”€â”€ test_sourcers.py                # Sourcer tests

- **YAKE** - Keyword extraction algorithmâ”œâ”€â”€ pyproject.toml                  # Project config & dependencies

- **Pydantic** - API data validationâ”œâ”€â”€ run.sh                          # Quick start script

â”œâ”€â”€ format.sh                       # Code formatting script

## Documentationâ”œâ”€â”€ README.md                       # This file

â”œâ”€â”€ DATA_LAKE.md                    # Data lake documentation

- **[Single Source of Truth](docs/SINGLE_SOURCE_OF_TRUTH.md)** - Config.json guideâ”œâ”€â”€ SOURCERS.md                     # Sourcer pipeline documentation

- **[Configuration Guide](docs/CONFIG_GUIDE.md)** - Complete referenceâ”œâ”€â”€ SUMMARY.md                      # Implementation summary

- **[Frontend Integration](docs/FRONTEND_GUIDE.md)** - API documentationâ”œâ”€â”€ ROADMAP.md                      # Future features

- **[Frontend Quickstart](docs/FRONTEND_QUICKSTART.md)** - Quick start guideâ””â”€â”€ QUICKREF.md                     # Quick reference

- **[Team System](docs/TEAM_SYSTEM_SUMMARY.md)** - Architecture overview```

- **[Data Lake](docs/DATA_LAKE.md)** - Storage design

- **[Keyword Extraction](docs/KEYWORD_EXTRACTION.md)** - NLP pipeline## Why FastAPI?



## License- **Fast:** High performance, on par with NodeJS and Go

- **Modern:** Python 3.11+ with type hints

MIT- **Auto docs:** Interactive API documentation out of the box

- **Standards-based:** Based on OpenAPI and JSON Schema
